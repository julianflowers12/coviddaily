---
title: "Daily covid literature update"
output: 
    github_document:  
    html_document:
      toc: yes
      toc_float: yes
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)

library(pacman)
p_load(tidyverse, jsonlite, gt, lubridate, textrank, ggraph, qgraph, topicmodels, DiagrammeR, lubridate, desctable, patchwork)
p_load_gh("trinker/entity")

```



```{r}
source("R/pubmedAbstractR.R")
source("R/text_summariser.R")
source("R/annotate_abstracts.R")
source("R/abstract_nounphrases.R")
source("R/abstract_topics.R")
source("R/topic_viz.R")
source("R/create_abstract_corpus.R")
source("R/create_abstract_clusters.R")
source("R/create_cluster_labels.R")
source("R/classify_abstracts.R")
source("R/classify_abstracts_1.R")
source("R/pubmed_extract_last_5_days.R")
```

• trained 2 models:
  + On full corpus of n medRxiv and bioRxiv covid papers <>
  + On m labelled abstracts as part of KLS daily briefings
• The former use a semi-supervised approach  - we used the process outlined by ...  - we tokenised and stemmed abstract texts and generated a tf-idf weighted document term matrix. We then applied dimensionality reduction using the tSNE algorithm and performed clustering on the resulting matrix using the DBSCAN clustering algorithm. This process is the slowest part of of the process and took ~ 4 hours and generated 41 distinct groups of abstracts.
• We autogenerated labels for each cluster by calculting the mean frequency and tf-idf of terms across the abstracts in each cluster, took the top 5 terms ranked by descending tf-idf and then ordered them by decending term frequency and joined them to give a cluster label.
• This process generated the following labels










## Pubmed search
```{r, echo=FALSE, cache = TRUE}
source("R/pubmed_extract.R")

pubmed_extract_5 <- pubmed_extract$abstracts %>%
  filter(pubDate >= today() - days(5))


pubmed_extract_5 %>%
  count(pubDate) %>%
  ggplot(aes(pubDate, n)) +
  geom_col() +
  ggtitle("Daily Pubmed abstracts") +
  labs(subtitle = paste0("From ", today() - days(5), " to ",  today())) +
  theme(plot.title.position = "plot")

```

```{r}

source("R/get_latest_medrxiv.R")

source("R/classify_abstracts.R")

```

```{r load data}

medrxiv <- get_latest_medRxiv()

medrxiv


```


## Number of abstracts

* Medrxiv since `r today() - days(5)`, `r nrow(medrxiv$latest_medRxiv)`
* Pubmed since `r today() - days(5)`, `r nrow(pubmed_extract_5)`


## Automating literature bulletins

* ~ 10,000 peer reviewed COVID publications in 2020.
* Also massive growth in use of pre-publication sites like medrxiv for COVID reports
* Keeping up with literature in real-time near impossible task
* Systematic review increasingly making use of software tools and using text analysis, natural language processing, data science and automation 

## Data science and literature reviewing

* Text = data
    + Text (abstract/ full text) can be converted into data formats which can be analysed
* Using all the data
    + Search - download - analyse + classify abstracts - filter - data extraction
    + Not: search - title screening - abstract screening - full text 
* Scripts and markdown not word
    + Can buil

## Process

* Create search strategy
* Search and download abstracts using APIs
* Filtering by date
* Use semi-supervised machine learning to build classification system for abstracts
    + Use nedrxiv corpus as training data (~3500 abstracts)
    + Use text analysis, dimensionality reduction and clustering to classify abstracts
    + Develop predictive model
    + Use to predict categories
* Use trainin


## Classify new pubmed and medrXiv abstracts

```{r}

test <- classify_abstracts(df = pubmed_extract_5)
test <- classify_abstracts_1(test)
medrxiv_new <- select(medrxiv$latest_medRxiv, title = rel_title, abstract = rel_abs, journal = rel_site, pmid = rel_doi, pubDate = rel_date)
test1 <- classify_abstracts(df = medrxiv_new)
test1 <- classify_abstracts_1(test1)
final <- test %>%
  bind_rows(test1) 

final %>%
  desctable() 

```


## Test categorisation

### PPE/ masks

```{r}

ppe <- final %>%
  filter(str_detect(class, "mask"))

ppe %>%
  select(title, class1, class, pubDate, journal) %>%
  knitr::kable()

```

### Socical distancing

```{r}

sd <- test %>%
  filter(str_detect(class, "distanc"))

sd %>%
  select(title, class1, class, pubDate,  journal) %>%
  knitr::kable()


```

### BCG

```{r}

bcg <- test %>%
  filter(str_detect(class, "[Bb][Cc][Gg]"))

bcg %>%
  select(title, class1, class, pubDate,  journal) %>%
  knitr::kable()


```


## Visualise

```{r fig.width=6}

a <- final %>%
  mutate(pre_pub = ifelse(str_detect(journal, "xiv"), "pre_print", "peer")) %>%
  count(pre_pub, pubDate, class) %>%
  ggplot(aes(pubDate, forcats::fct_rev(class), size = n,  colour = n)) +
  geom_point() +
  scale_color_viridis() +
  labs(y = "") + 
  facet_wrap(~pre_pub)
  
 b <- final %>%
  mutate(pre_pub = ifelse(str_detect(journal, "xiv"), "pre_print", "peer")) %>%
  count(pre_pub, pubDate, class1) %>%
  ggplot(aes(pubDate, forcats::fct_rev(class1), size = n,  colour = n)) +
  geom_point() +
  scale_color_viridis() +
  labs(y = "") + 
  facet_wrap(~pre_pub) 

a / b

```


## 2nd stage clustering

```{r}

new <- final %>%
  filter(class %in% c("2-infect-result-covid-19", "patient-diseas-covid-19-result"))

newcorp <- create_abstract_corpus(new)
newclust <- create_abstract_cluster(newcorp$corpus, minPts = 10)

newclust$cluster_count
newclust$cluster_size

newlabels <- create_cluster_labels(newcorp$corpus, newclust$clustering)

newlabels$labels

newlabels$results %>%
  filter(cluster != 0) %>%
  ggplot(aes(X1, X2, group = cluster, colour = factor(cluster))) +
  geom_point() +
  ggrepel::geom_text_repel(data = newlabels$plot, aes(x = medX, y = medY, label = clus_names), colour = "black") +
  stat_ellipse()
  

final1 <- newlabels$results %>%
  rename(clus_names_1 = clus_names) %>%
  full_join(final, by = c("pmid.value" = "pmid"))


final1 <- final1 %>%
  select(pmid.value, cluster, class, clus_names_1, title, abstract, journal, pubDate, absText)

final1 %>%
  count(clus_names_1)

final1 %>%
  filter(clus_names_1 %in% c("consult-patient-pandem-covid-19"))

```

```{r fig.height=4}

final1 %>%
  count(class, clus_names_1) %>%
  filter(!is.na(clus_names_1)) %>%
  ggplot(aes(clus_names_1, forcats::fct_rev(class), fill = n)) +
  geom_tile() +
  scale_x_discrete(position = "top") +
  theme(axis.text.x.top = element_text(angle = 90, hjust = 0)) +
  labs(x = "subcategory" ,
       y = "category") +
  scale_fill_viridis()



```


## Abstract summaries

```{r summaries, cache = TRUE}

abstract_summary <- final1 %>%
  mutate(text_summ = map_chr(absText, possibly(text_summariser, otherwise = NA_real_)))

abstract_summary %>%
  select(pmid.value, title, text_summ, class, clus_names_1) %>%
  sample_n(15) %>%
  DT::datatable()

```

## Using NLP


```{r annotation, cache=TRUE}

anno <- annotate_abstracts(abstract_summary$text_summ, pmid = final$pmid)

ids <- final %>%
  filter(class %in% c("sar-2-infect-covid-19", "patient-diseas-covid-19-result")) %>%
  pull("pmid")

np <- abstract_nounphrases(anno)

anno1 <- np %>%
  filter(doc_id %in% ids)

length(unique(np$doc_id))

```



```{r}


np_summary <- np %>% 
  group_by(doc_id) %>%
  filter(!is.na(term)) %>%
  mutate(summ = paste(term, collapse = "; ")) %>%
  select(doc_id, summ) %>%
  distinct()

np_summary %>%
  head(10) %>%
  knitr::kable()


```

## Topic visualisation

```{r}

abs_topics <- abstract_topics(k = 15, x = anno1)

# abs_topics$scores %>%
#   count(doc_id)

#abs_topics$scores
#abs_topics$terms

map(1:15, ~(abstract_topic_viz(x = np, m = abs_topics$model, scores = abs_topics$scores, n = .x)))

```

```{r, eval=FALSE}

topics <- abs_topics$terms %>%
  enframe() %>%
  mutate(topic = as.numeric(ifelse(str_detect(name, "00"), str_remove_all(name, "topic_00"), str_remove_all(name, "topic_0"))))

final1 <- abs_topics$scores %>%
  left_join(final1, by = c("doc_id" = "pmid.value")) %>%
  select(doc_id, topic, title, abstract, journal, pubDate, class) %>%
  left_join(topics, by = c("topic" = "topic")) %>%
  unnest("value") %>%
  group_by(doc_id) %>%
  mutate(keywords = paste(term, collapse = ", ")) %>%
  select(-c(term, prob)) %>%
  distinct() %>%
  full_join(final1)
  
  
final1 <- final1 %>%
  ungroup() %>%
  mutate(id = row_number())



```



```{r eval = FALSE}
library(entity)

locs <- entity::location_entity(final1$absText) %>%
  enframe() %>%
  unnest("value") %>%
  distinct() %>%
  group_by(name) %>%
  mutate(locations = paste(value, collapse = "; ")) %>%
  select(-value) %>%
  distinct() %>%
  full_join(final1, by = c("name" = "id"))
  


final2  <- final1 %>%
  left_join(locs, by = c("id" = "name")) %>%
  left_join(np_summary)

head(final2, 20) %>%
  knitr::kable()


```

