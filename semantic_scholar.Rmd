---
title: "Semantic sholar covid papers (CORD19)"
output:
  github_document: default
  powerpoint_presentation: default
  slidy_presentation: default
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)
library(pacman)
p_load(tidylo, tidyverse, data.table, broom, tidytext)

```


## Semantic scholar covid dataset

* Semantic scholar is...
* It publishes a COVID dataset which can be downloaded for analysis https://www.semanticscholar.org/cord19
* https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/latest/metadata.csv


## Exploring


```{r download-cord19, cache = TRUE}
semantic_scholar <- function(){

cat("Please wait... file size > 1GB")

library(data.table)
library(lubridate)
library(tidyverse)


## extract semantic scholar covid data
metadata <- fread("https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/latest/metadata.csv")
}
library(tictoc)

source("R/semantic_scholar.R")
tic()
ss <- semantic_scholar()
toc()

```

* We can download the whole corpus of `r nrow(ss$metadata)` abstracts
* I have built two classification algorithms:
    + One trained on clustering 4000 medRxiv abstracts
    + One trained on Nicola's classification of  ~ 2000 abstracts
* The `semantic_scholar` function downloads the data, creates some plots, extracts the most recent 5 days worth of abstracts, adds the two classifications and outputs an interactive table
* This process takes ~ 3-4 minutes

## Plot abstract frequency

```{r}
str(ss)

```








## Journal frequency


```{r journal-freq}

setDT(ss$sem_class)[, .N, by = .(class, class1)] %>%
  bind_log_odds(class, class1,  N) %>%
  arrange(class) %>%
  tail(20)


```

## Topic models

### STM

```{r stm, cache=TRUE, results="hide"}

library(stm)
library(quanteda)

corp1 <- corpus(ss, text_field = "abstract")
corp_samp <- corpus_sample(corp1, 200000)
dfm1 <- tokens_remove(tokens(corp_samp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE), stopwords("english")) %>% tokens_select(., pattern = c("de", "la"), selection = "remove", valuetype = "regex", case_insensitive = TRUE, padding = FALSE, window = 0, min_nchar = 1, max_nchar = 2, startpos = 1, endpos = 2, verbose = FALSE) 
dfm1 <- dfm(dfm1)
#dfm1 <- dfm_tfidf(dfm1)

stm <- convert(dfm1, to = "stm")
stm_m <- prepDocuments(stm$documents, stm$vocab, stm$meta)


```p

```{r stm-fit}

fit_stm <- stm(stm$documents, stm$vocab, data = stm$meta, K = 50, init.type = "Spectral")

assign_labels <- tidy(fit_stm, "beta") %>%
  group_by(topic) %>%
  top_n(5, beta)

topic_labels <- assign_labels %>%
  arrange(topic, -beta) %>%
  mutate(topic_label = paste0(term, collapse = "-")) %>%
  select(topic, topic_label) %>%
  distinct()


assign_topics <- tidy(fit_stm, matrix = "gamma", document_names = stm_m$meta$pubmed_id) %>%
  group_by(topic, document) %>%
  mutate(maxdoc = max(gamma)) %>%
  ungroup() %>%
  arrange(document, -maxdoc) %>%
  group_by(document) %>%
  slice(1)

assign <- assign_topics %>%
  left_join(docvars(corp_samp), by = c("document" = "pubmed_id")) %>%
  left_join(topic_labels) %>%
  filter(publish_time > "2020-01-31") %>%
  arrange(desc(publish_time))

glimpse(assign)
  
assign %>%
    ungroup() %>%
    count(topic_label, sort = TRUE)
```

```{r stm-time}

model.labels <- labelTopics(fit_stm, 1:30)
stm_m$meta$date <- as.numeric(stm_m$meta$publish_time)

model_est <- estimateEffect(1:30 ~ s(date), fit_stm, meta = stm_m$meta)

par(mfrow = c(3,3))

for(i in 1:30){
  
  plot(model_est, "date", method = "continuous", topics = i, main =  paste0(model.labels$prob[i, 1:3], collapse = ", "), printLegend = F)
  
  
}


```

```{r}

```

