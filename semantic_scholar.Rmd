---
title: "Semantic sholar covid papers (CORD19)"
output:
  github_document: default
  powerpoint_presentation: default
  slidy_presentation: default
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)
library(pacman)
p_load(tidylo, tidyverse, data.table, broom, tidytext)

```


## Semantic scholar covid dataset

* Semantic scholar is...
* It publishes a COVID dataset which can be downloaded for analysis https://www.semanticscholar.org/cord19
* https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/latest/metadata.csv


## Exploring


```{r download-cord19, cache = TRUE}

library(tictoc)

source("R/semantic_scholar.R")
tic()
ss <- semantic_scholar()
toc()

```

* We can download the whole corpus of `r nrow(ss$metadata)` abstracts
* I have built two classification algorithms:
    + One trained on clustering 4000 medRxiv abstracts
    + One trained on Nicola's classification of  ~ 2000 abstracts
* The `semantic_scholar` function downloads the data, creates some plots, extracts the most recent 5 days worth of abstracts, adds the two classifications and outputs an interactive table
* This process takes ~ 3-4 minutes

## Plot abstract frequency

```{r}

ss$plot_meta

```


## Cluster frequency

```{r clus_freq, fig.height=4, fig.width=6}

sem_class_count <- ss$sem_class %>%
  count(class, class1)

sem_odds <- sem_class_count %>%
  bind_log_odds(class1, class, n, unweighted = TRUE)


sem_class_count %>%
  ggplot(aes(class1, forcats::fct_rev(class), size = n, colour = n)) +
  geom_point() +
  viridis::scale_color_viridis(option = "cividis") +
  scale_x_discrete(position = "top") +
  ggthemes::theme_fivethirtyeight() +
  theme(axis.text.x.top = element_text(angle = 90, hjust = .01)) +
  labs(title = paste("Clustering of ", nrow(ss$sem_class), " COVID-related \nabstracts since 1st Jan 2020"))

```





## Journal frequency


```{r journal-freq}

setDT(ss$sem_class)[, .N, by = .(class, class1)] %>%
  bind_log_odds(class, class1,  N) %>%
  arrange(class) %>%
  tail(20)


```

## Topic models

### STM

```{r stm, cache=TRUE, results="hide"}

library(stm)
library(quanteda)

corp1 <- corpus(ss$sem_class, text_field = "abstract")
dfm1 <- dfm(corp1, remove = stopwords("en"), remove_punct = TRUE, remove_numbers = TRUE, stem = TRUE, remove_symbols = TRUE, ngrams = 1:2)
#dfm1 <- dfm_tfidf(dfm1)

stm <- convert(dfm1, to = "stm")
stm_m <- prepDocuments(stm$documents, stm$vocab, stm$meta)

```

```{r stm-fit}

fit_stm <- stm(stm_m$documents, stm_m$vocab, data = stm_m$meta, K = 30, init.type = "Spectral")

assign_labels <- tidy(fit_stm, "beta") %>%
  group_by(topic) %>%
  top_n(5, beta)

topic_labels <- assign_labels %>%
  arrange(topic, -beta) %>%
  mutate(topic_label = paste0(term, collapse = "-")) %>%
  select(topic, topic_label) %>%
  distinct()


assign_topics <- tidy(fit_stm, matrix = "gamma", document_names = stm_m$meta$pubmed_id) %>%
  group_by(topic, document) %>%
  mutate(maxdoc = max(gamma)) %>%
  ungroup() %>%
  arrange(document, -maxdoc) %>%
  group_by(document) %>%
  slice(1)

assign <- assign_topics %>%
  left_join(ss$sem_class, by = c("document" = "pubmed_id")) %>%
  select(title, publish_time, topic, class, class1) %>%
  left_join(topic_labels) %>%
  filter(publish_time > "2020-01-31") %>%
  arrange(desc(publish_time))

head(assign)
  

```

```{r stm-time}

model.labels <- labelTopics(fit_stm, 1:30)
stm_m$meta$date <- as.numeric(stm_m$meta$publish_time)

model_est <- estimateEffect(1:30 ~ s(date), fit_stm, meta = stm_m$meta)

par(mfrow = c(3,3))

for(i in 1:30){
  
  plot(model_est, "date", method = "continuous", topics = i, main =  paste0(model.labels$prob[i, 1:3], collapse = ", "), printLegend = F)
  
  
}


```

```{r}

```

